#!/usr/bin/env bash
### DESCRIPTION: Provide structured feedback from subagents to the leader

set -euo pipefail

SCRIPT_NAME="aimeta feedback"
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
FEEDBACK_DIR="${HOME}/.local/share/aimeta"
FEEDBACK_FILE="${FEEDBACK_DIR}/feedback.md"

# Colors for output
if [[ -t 1 ]]; then
  BOLD='\033[1m'
  GREEN='\033[32m'
  YELLOW='\033[33m'
  RED='\033[31m'
  RESET='\033[0m'
else
  BOLD=''
  GREEN=''
  YELLOW=''
  RED=''
  RESET=''
fi

# Global variables for feedback
ACTION=""
PROMPT_RATING=""
MESSAGE_RATING=""
REASON=""

usage() {
  cat <<EOF
${SCRIPT_NAME} - Provide structured feedback from subagents to the leader

USAGE:
  ${SCRIPT_NAME} [options]

OPTIONS:
  --paths                 Show feedback file paths and exit
  --stop                  Stop current operation
  --continue              Continue current operation
  --start                 Start new operation
  --prompt-rating N       Rating for the prompt (1-5 scale)
  --message-rating N      Rating for the message (1-5 scale)
  --reason TEXT           Detailed reason for feedback
  --help-best-practices   Show comprehensive best practices for providing feedback
  --help, -h              Show this help message

DESCRIPTION:
  Provides structured feedback from subagents to the leader agent with
  action indicators, rating system, and detailed reasons. Includes
  comprehensive guidance on how to provide effective feedback for
  improving task assignment and orchestration.

EXAMPLES:
  ${SCRIPT_NAME} --paths
  ${SCRIPT_NAME} --continue --prompt-rating 4 --message-rating 5 --reason "Good progress, continue as planned"
  ${SCRIPT_NAME} --stop --prompt-rating 1 --message-rating 2 --reason "Critical issue encountered"
  ${SCRIPT_NAME} --start --prompt-rating 5 --message-rating 4 --reason "Ready to begin new task"
  ${SCRIPT_NAME} --help-best-practices
EOF
}

# Validate rating (must be 1-5)
validate_rating() {
  local rating="$1"
  local name="$2"

  if ! [[ "$rating" =~ ^[1-5]$ ]]; then
    echo "Error: ${name} rating must be between 1 and 5" >&2
    exit 1
  fi
}

# Parse command line arguments
parse_args() {
  local action_set=false
  local rating_set=false

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --paths)
        show_paths
        exit 0
        ;;
      --stop)
        if [[ -n "$ACTION" ]]; then
          echo "Error: Only one action flag can be specified" >&2
          exit 1
        fi
        ACTION="stop"
        action_set=true
        shift
        ;;
      --continue)
        if [[ -n "$ACTION" ]]; then
          echo "Error: Only one action flag can be specified" >&2
          exit 1
        fi
        ACTION="continue"
        action_set=true
        shift
        ;;
      --start)
        if [[ -n "$ACTION" ]]; then
          echo "Error: Only one action flag can be specified" >&2
          exit 1
        fi
        ACTION="start"
        action_set=true
        shift
        ;;
      --prompt-rating)
        if [[ -n "$PROMPT_RATING" ]]; then
          echo "Error: --prompt-rating can only be specified once" >&2
          exit 1
        fi
        PROMPT_RATING="$2"
        validate_rating "$PROMPT_RATING" "prompt"
        shift 2
        ;;
      --message-rating)
        if [[ -n "$MESSAGE_RATING" ]]; then
          echo "Error: --message-rating can only be specified once" >&2
          exit 1
        fi
        MESSAGE_RATING="$2"
        validate_rating "$MESSAGE_RATING" "message"
        shift 2
        ;;
      --reason)
        if [[ -n "$REASON" ]]; then
          echo "Error: --reason can only be specified once" >&2
          exit 1
        fi
        REASON="$2"
        shift 2
        ;;
      --help-best-practices)
        show_best_practices
        exit 0
        ;;
      --help|-h)
        usage
        exit 0
        ;;
      *)
        echo "Error: Unknown option '$1'" >&2
        usage >&2
        exit 1
        ;;
    esac
  done

  # Validate required arguments
  if [[ -z "$ACTION" ]]; then
    echo "Error: An action flag (--stop, --continue, or --start) is required" >&2
    usage >&2
    exit 1
  fi

  # If ratings are provided, validate they are in 1-5 range
  if [[ -n "$PROMPT_RATING" ]]; then
    validate_rating "$PROMPT_RATING" "prompt"
  fi

  if [[ -n "$MESSAGE_RATING" ]]; then
    validate_rating "$MESSAGE_RATING" "message"
  fi
}

# Show feedback file locations
show_paths() {
  cat <<EOF
Feedback directory: ${FEEDBACK_DIR}
Feedback file: ${FEEDBACK_FILE}
EOF
}

# Format rating as stars
format_rating() {
  local rating="$1"
  local stars=""

  for ((i=1; i<=5; i++)); do
    if [[ $i -le $rating ]]; then
      stars="${stars}★"
    else
      stars="${stars}☆"
    fi
  done

  echo "$stars"
}

# Display feedback in a structured format
display_feedback() {
  local action_color=""
  case "$ACTION" in
    stop) action_color="${RED}" ;;
    continue) action_color="${GREEN}" ;;
    start) action_color="${YELLOW}" ;;
  esac

  echo -e "${BOLD}Subagent Feedback:${RESET}"
  echo -e "  ${action_color}Action: ${ACTION^}${RESET}"

  if [[ -n "$PROMPT_RATING" ]]; then
    echo -e "  Prompt Rating: $(format_rating "$PROMPT_RATING") (${PROMPT_RATING}/5)"
  fi

  if [[ -n "$MESSAGE_RATING" ]]; then
    echo -e "  Message Rating: $(format_rating "$MESSAGE_RATING") (${MESSAGE_RATING}/5)"
  fi

  if [[ -n "$REASON" ]]; then
    echo -e "  Reason: $REASON"
  fi
}

# Append feedback to local feedback log
append_feedback() {
  mkdir -p "$FEEDBACK_DIR"

  {
    echo "## $(date -u '+%Y-%m-%dT%H:%M:%SZ')"
    echo "Action: ${ACTION}"

    if [[ -n "$PROMPT_RATING" ]]; then
      echo "Prompt Rating: ${PROMPT_RATING}/5"
    fi

    if [[ -n "$MESSAGE_RATING" ]]; then
      echo "Message Rating: ${MESSAGE_RATING}/5"
    fi

    if [[ -n "$REASON" ]]; then
      echo "Reason: ${REASON}"
    fi

    echo ""
  } >> "$FEEDBACK_FILE"
}

# Display best practices for providing effective feedback
show_best_practices() {
  cat <<EOF
${SCRIPT_NAME} - Best Practices for Providing Effective Feedback

FEEDBACK CATEGORIES AND GUIDANCE:

1. task-capability-match
   - Purpose: Assess whether the assigned task matched the agent's capabilities
   - Focus: Did the agent have the right skills and knowledge for this task?
   - Good feedback: "The task required specialized knowledge in Kubernetes that I possess, making it a good capability match"
   - Bad feedback: "I don't know what I'm doing" (vague, doesn't explain capability mismatch)

2. agent-purpose-alignment
   - Purpose: Evaluate if the task aligned with the agent's core purpose
   - Focus: Was this task appropriate for what the agent is designed to do?
   - Good feedback: "This task aligned well with my purpose of optimizing system performance"
   - Bad feedback: "This wasn't my job" (complaining without constructive insight)

3. skill-utilization
   - Purpose: Check if the agent's specialized skills were properly utilized
   - Focus: Were the right skills applied effectively to the task?
   - Good feedback: "My expertise in database optimization was effectively utilized for query tuning"
   - Bad feedback: "I used basic skills when I could have used advanced ones" (doesn't explain why)

4. task-complexity
   - Purpose: Assess if the task complexity was appropriate
   - Focus: Was the task too simple, too complex, or just right?
   - Good feedback: "The task complexity was well-balanced - challenging but achievable"
   - Bad feedback: "This was too hard" (doesn't provide specific complexity issues)

5. assignment-effectiveness
   - Purpose: Evaluate the overall effectiveness of the task assignment
   - Focus: How well did the assignment process work?
   - Good feedback: "The assignment process was effective, and I received clear instructions"
   - Bad feedback: "The assignment was bad" (vague, lacks specific improvement suggestions)

RETROSPECTIVE APPROACH:
- Feedback should be constructive and focused on improvement
- Provide specific examples rather than general complaints
- Explain the impact of issues encountered
- Suggest concrete improvements for future assignments
- Focus on what worked well and what could be improved

EXAMPLES OF EFFECTIVE FEEDBACK:

✅ GOOD: "The task matched my capabilities well. However, I noticed the complexity could be better balanced - some aspects were too simple while others required advanced knowledge."

✅ GOOD: "This task aligned with my purpose of code analysis. My skill in static analysis was effectively utilized, but I could have used more specialized tools for performance profiling."

❌ BAD: "I don't like this task" (lacks specificity and constructive feedback)

❌ BAD: "It was too hard" (doesn't explain the specific challenges or how complexity could be improved)

HOW FEEDBACK HELPS:
- Improves task assignment algorithms
- Helps match agents to appropriate tasks
- Refines complexity assessment
- Enhances overall orchestration system
- Creates better agent-task alignment

When providing feedback, be specific, constructive, and focus on actionable improvements that will help the system learn and improve over time.
EOF
}

# Main function
main() {
  parse_args "$@"
  display_feedback
  append_feedback
}

main "$@"
